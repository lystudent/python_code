# -*- coding: utf-8 -*-
# @Time    : 2019-04-05 00:21
# @Author  : liuyanming
# @Email   : 1420743191@qq.com
# @File    : usgs_crwa.py
# @Software: PyCharm

# -*- coding: UTF-8 -*-

#   å¯¼å…¥httpè¯·æ±‚åŒ… -->ç”¨äºhttpè¯·æ±‚
import requests
#   å¯¼å…¥ç±»é€‰æ‹©å™¨è§£æåŒ… -->ç”¨äºè¯·æ±‚åç»“æœè§£æ
from bs4 import BeautifulSoup
#   å¯¼å…¥jsonåŒ… -->ç”¨äºjsonæ ¼å¼æ“ä½œ
import json
#   æ­£åˆ™åŒ…
import re
#   æ—¶é—´å‡½æ•°-->ä¼‘çœ 
import time
#   é—­åŒ…
from contextlib import closing
#   å¼•å…¥ç³»ç»ŸåŒ…-->ä¸ºjavaæä¾›å‡½æ•°è°ƒç”¨
import sys
from typing import Dict, Tuple, List, Optional

'''
    @author chenbaining
    Date: 2019/3/16 ä¸Šåˆ9:17
    @since Python 3.6
    èƒŒæ™¯ï¼šä»ers.cr.usgs.govç½‘ç«™ä¸‹è½½å“¨å…µé¥æ„Ÿå½±åƒæ•°æ®
    å®ç°ï¼š1.ç½‘ç«™åªå…è®¸ç™»é™†åé€‰æ‹©å½±åƒä¸‹è½½ï¼Œæ‰€ä»¥éœ€è¦å…ˆè¿›è¡Œç™»é™†
         1.1 ç™»é™†éœ€è¦302è·³è½¬,ç¦ç”¨302è‡ªåŠ¨è·³è½¬
         2.é€‰æ‹©å½±åƒåŒºåŸŸå’Œå½±åƒæ•°æ®çš„æ—¶é—´ï¼Œè¿›è¡Œæäº¤åˆ°ä¸‹ä¸€æ­¥
         3.é€‰æ‹©å½±åƒç±»å‹è¿›è¡Œä¸‹ä¸€æ­¥
         4.é€‰æ‹©äº‘é‡è¿›è¡Œä¸‹ä¸€æ­¥ 
         5.æäº¤æ•°æ®ï¼Œé€‰æ‹©å½±åƒæ•°æ®æ ¼å¼è¿›è¡Œä¸‹è½½ [ä¸‹è½½æ—¶é—´è¿‡é•¿ï¼Œéœ€è¦æœ‰æç¤º]
         6.ä¸‹è½½ [éœ€è¦å¾ªç¯ä¸‹è½½æ‰€æœ‰å½±åƒæ•°æ®]
         ä»¥ä¸Šæ‰€æœ‰æ­¥éª¤éƒ½ä¸ºå•ç‹¬çš„defï¼Œæœ€åæä¾›ä¸€ä¸ªç»Ÿä¸€å‡ºå£
    å¯åŠ¨ï¼šæ–‡ä»¶å†…ï¼Œè¯·ç›´æ¥ç‚¹å‡»"__main__"è¿è¡Œå³å¯
         javaè°ƒç”¨ï¼Œè¯·è¿è¡Œ"img_main"æ–¹æ³•ï¼Œå»æ‰sys.argv[1]ç­‰ä¼—å¤šå‚æ•°æ³¨é‡Š
'''

#   å…¨å±€request è‡ªåŠ¨ç»´æŠ¤cookie
conn: requests.Session = requests.session()


#   ç™»é™†æ¨¡å—-->å½“å‰é¡¹ç›®ç™»é™†åˆ†ä¸ºä¸‰å—
#   1.å…ˆè·³è½¬åˆ°ç™»é™†é¡µæ‹¿token
#   2.åå°†æ‹¿åˆ°çš„tokenå’Œè´¦å·å¯†ç è¿›è¡Œpostæäº¤
#   3.æˆåŠŸåæ¨¡æ‹Ÿ302è·³è½¬ï¼Œè¿›è¡Œé¡µé¢è¯·æ±‚ï¼Œåˆ¤æ–­é¡µé¢è¿”å›å€¼
def login(request_username: str, request_password: str) -> Optional[str]:
    assert isinstance(request_username, str) and isinstance(request_password, str)
    result_str = None
    #   è·³è½¬åˆ°ç™»é™†é¡µ
    res = conn.get('https://ers.cr.usgs.gov/login')
    #   è·å–è¯·æ±‚ç»“æœï¼Œä»¥htmlå½¢å¼è§£æ
    soup = BeautifulSoup(res.text, 'html.parser')
    # è·å–é¡µé¢ä¸­tokenå€¼
    token = soup.select('#csrf_token')[0]['value']
    # print(token)

    headers = {
        "Host": "ers.cr.usgs.gov",
        "Content-Type": "application/x-www-form-urlencoded",
        "Referer": "https://ers.cr.usgs.gov/login",
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36"
    }
    #   ç™»é™†æ‰€éœ€è¦çš„è¯·æ±‚å‚æ•°
    d = {'username': request_username, 'password': request_password, 'csrf_token': token}
    #   è¿›è¡Œå‚æ•°æäº¤ ç¦æ­¢302è·³è½¬ï¼Œæ‰‹åŠ¨å¯åŠ¨è·³è½¬
    res = conn.post('https://ers.cr.usgs.gov/login/', headers=headers, data=d, allow_redirects=False)
    #  å¦‚æœçŠ¶æ€ä¸º302ï¼Œæ­£ç¡®
    if res.status_code == 302:
        res = conn.get('https://ers.cr.usgs.gov/')
        soup = BeautifulSoup(res.text, 'html.parser')
        result_str = soup.select('div#credentialContainer > span')[0].text.strip()
    # è¿”å›å€¼
    return result_str


#   é˜¶æ®µ1-->ä¸Šä¼ shpfileã€è½¬æ¢åæ ‡ç‚¹ã€è®¾ç½®æ—¶é—´
def stage_1(request_file_path: str, request_data_start: str, request_data_end: str) -> None:
    assert isinstance(request_file_path, str) and isinstance(request_data_start, str)
    assert isinstance(request_data_end, str)
    #   é¦–å…ˆéœ€è¦ä¼˜å…ˆè®¿é—®ä¸Šä¼ é¡µé¢è·å–token
    res = conn.get('https://earthexplorer.usgs.gov/upload/shapefile')
    #   æ­£åˆ™åŒ¹é…åˆ°token
    pattern = re.compile(r'X-Csrf-Token(.*)\'')
    re_result = pattern.search(res.text).group()
    token = re_result.replace('X-Csrf-Token', '').replace('\'', '').replace(',', '').strip()
    print('token:', token)

    # å› ä¸ºæ˜¯jså¼‚æ­¥åŠ è½½ï¼Œå¿…é¡»åŠ ä¸ŠX-Requested-With å˜æˆXHRè¯·æ±‚ã€‚X-Csrf-Tokenæ˜¯tokenï¼Œé¡µé¢ä¸­å¸¦çš„
    headers = {
        'X-Requested-With': 'XMLHttpRequest',
        'X-Csrf-Token': token,
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'
    }
    files = {'files': open(request_file_path, 'rb')}  # æ–‡ä»¶
    res = conn.post('https://earthexplorer.usgs.gov/upload/shapefile', headers=headers, files=files)

    print('æ‰“å°', res, res.text)

    json_data = json.loads(res.text)
    #   è¯»å–å…¶ä¸­çš„åæ ‡ç‚¹
    dis = [(item.get('lat', 'NA'), item.get('lng', 'NA')) for item in json_data['coordinates']]
    #   å¾ªç¯è¾“å‡ºåæ ‡ç‚¹å¹¶å››èˆäº”å…¥ä¿ç•™å°æ•°ç‚¹åå››ä½
    test_dict = {"tab": 1, "destination": 2, "coordinates": None, "format": "dms", "dStart": request_data_start,
                 "dEnd": request_data_end, "searchType": "Std", "num": "100", "includeUnknownCC": "1", "maxCC": 100,
                 "minCC": 0, "months": ["", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11"],
                 "pType": "polygon"}

    point_list: List = [
        {'c': str(index),
         'a': f"{float(elem[0]):.4f}",
         'o': f"{float(elem[1]):.4f}"
         }
        for index, elem in enumerate(dis)
    ]
    test_dict['coordinates'] = point_list
    request_json = json.dumps(test_dict)

    print(request_json)

    #   coordinatesåæ ‡ç‚¹éœ€è¦åŠ¨æ€å¡«å†™ï¼Œå‰©ä¸‹ä¸ºå›ºå®šæ ¼å¼
    d = {
        'data': request_json
    }
    res = conn.post('https://earthexplorer.usgs.gov/tabs/save', data=d)
    status = res.text
    if int(status) == 1:
        print(f'stage_1 çŠ¶æ€ä¸º{"Success"}')
    else:
        print(f'stage_1 çŠ¶æ€ä¸º{"Failure"}')


#   é˜¶æ®µ2-->é€‰æ‹©æ•°æ®é›† å“¨å…µå½±åƒ
def stage_2() -> None:
    # res = conn.post('https://earthexplorer.usgs.gov/ajax/datasetnotification',
    #                 data={'collection_id': '10880'})
    # print('stage_2', res.text)
    #   å“¨å…µå½±åƒï¼Œå›ºå®šå†™æ³•ï¼Œå¦‚æœéœ€è¦å…¶ä»–å½±åƒæ•°æ®ï¼Œè¯·ä»get_img_listæ–¹æ³•ä¸­è·å–
    request_data = '{"tab":2,"destination":3,"cList":["10880"],"selected":10880}'
    d = {
        'data': request_data
    }
    res = conn.post('https://earthexplorer.usgs.gov/tabs/save', data=d)
    status = res.text
    if int(status) == 1:
        print(f'stage_2 çŠ¶æ€ä¸º{"Success"}')
    else:
        print(f'stage_2 çŠ¶æ€ä¸º{"Failure"}')


#   é˜¶æ®µ3-->é€‰æ‹©äº‘é‡
def stage_3(request_cloud_amount: str) -> None:
    #   äº‘é‡é€‰æ‹© select_18696_5=""ä¸º ALLã€select_18696_5="0"-->10% ã€select_18696_5="1"-->20% ã€select_18696_5="9"-->100%
    request_data = '{"tab":3,"destination":4,"criteria":{"10880":{"select_18696_5":["' + request_cloud_amount + '"],"select_18697_3":[""],"select_22067_3":[""]}},"selected":"10880"}'
    d = {
        'data': request_data
    }
    res = conn.post('https://earthexplorer.usgs.gov/tabs/save', data=d)
    status = res.text
    if int(status) == 1:
        print(f'stage_3 çŠ¶æ€ä¸º{"Success"}')
    else:
        print(f'stage_3 çŠ¶æ€ä¸º{"Failure"}')


#   å½±åƒåˆ—è¡¨å¯»æ‰¾å½±åƒä¸‹è½½ï¼Œå¾ªç¯è·å–åˆ°å½±åƒ
def stage_4_img_list() -> List:
    result_img_id_list = []
    current_page_num = 1
    total_page_num = 1
    while current_page_num <= total_page_num:
        d = {
            'collectionId': '10880',
            'pageNum': current_page_num
        }
        res = conn.post('https://earthexplorer.usgs.gov/result/index', data=d)
        soup = BeautifulSoup(res.text, 'html.parser')
        #  è·å–åˆ°çš„idæ”¾å…¥é›†åˆä¸­
        for img_id in soup.select('tbody > tr'):
            result_img_id_list.append(img_id['data-entityid'])

        # è·å–æ€»é¡µæ•°
        for request_total_page in soup.select('select#pageSelector_10880 >option'):
            total_page_num = int(request_total_page.text.strip())

        # å½“å‰é¡µéœ€è¦åŠ 1
        current_page_num += 1
    return result_img_id_list


#   ä¸‹è½½å½±åƒ
def download_img(request_img_id_list: List) -> None:
    assert isinstance(request_img_id_list, List)
    for img_id in request_img_id_list:
        url = 'https://earthexplorer.usgs.gov/download/options/10880/' + img_id
        print('\rå½“å‰å½±åƒé“¾æ¥[%s]' % url)
        res = conn.post(url)
        soup = BeautifulSoup(res.text, 'html.parser')
        # soup = BeautifulSoup(open('download_img.html').read(), 'html.parser')
        #   éœ€è¦è§£ææ•°æ®æ—¶ï¼Œå…ˆä¸‹è½½åˆ°æœ¬åœ°ï¼Œæœ¬åœ°è§£æé€šè¿‡åï¼Œå†æ³¨é‡Šæ‰å½“å‰ä»£ç 
        # fw = open("download_img.html", 'w')
        # fw.write(res.text)

        n = 0
        for img_url in soup.select('div > input'):
            onclick_url = img_url['onclick']
            # æ ¹æ®æ•°æ®åˆ†æç»„è¦æ±‚è·å–STANDARDæ•°æ®
            if onclick_url.find('STANDARD') > 0:
                result_url = onclick_url.replace('window.location=', '').replace('\'', '')
                #    è¯·æ±‚url ç¦æ­¢è·³è½¬
                res = conn.post(result_url, allow_redirects=False)
                if res.status_code == 302:
                    # è·å–é‡å®šå‘çš„åœ°å€
                    location = res.headers['Location']
                    print('å½“å‰å½±åƒè·³è½¬åçš„åœ°å€[%s]' % location)
                    # ä»åœ°å€ä¸­ï¼Œæˆªå–æ–‡ä»¶çš„åç§°ï¼Œä½œä¸ºä¸‹è½½åæ–‡ä»¶çš„åç§°
                    file_name = location[0:location.find('?')].split('/')[-1]
                    #   æ‰§è¡Œé‡å®šå‘åçš„åœ°å€ï¼Œè¯·æ±‚æ•°æ®
                    download_redirect(file_name, location, n + 1)
                    break
        #   ç®€å•åé˜²çˆ¬
        time.sleep(6)
    print("ä»»åŠ¡æ‰§è¡Œå®Œæ¯•")


#   ğŸŒŸè·å–æ‰€æœ‰ç±»å‹å½±åƒåˆ—è¡¨[æš‚æ—¶æ²¡æœ‰ç”¨][æœ‰ç©ºè…¾å‡ºæ—¶é—´å¤„ç†æ­¤åˆ—è¡¨å¯å®ç°ä¸‹è½½å…¶ä»–ç±»å‹æ•°æ®]
#   å¦‚æœéœ€è¦è¿›è¡Œå½±åƒè§£æï¼Œå‚è€ƒsampleList.txtæ–‡ä»¶ï¼Œæˆ–è€…è¯·æ±‚https://earthexplorer.usgs.gov/ajax/getdatasetsåœ°å€
def get_img_list() -> None:
    # res = conn.get('https://earthexplorer.usgs.gov/ajax/getdatasets')
    # soup = BeautifulSoup(res.text, 'html.parser')

    soup = BeautifulSoup(open('sampleList.txt').read())
    # for root in soup.select('ul#dataset-menu'):
    #     print(root)
    #     for img_list in root.select('li'):
    #         try:
    #             print(img_list.select('span[class="folder"] > div[class="categoryHitArea'))
    #         except:
    #             pass
    result = soup.find_all('script')[0].text.strip()
    # åŒ¹é…å¼€å§‹æ—¶çš„ç´¢å¼•ï¼Œè·å–å­—ç¬¦ä¸²é•¿åº¦ï¼Œè‡ªåŠ¨å»æ‰EE.dataset.datasets=
    star_index = len('EE.dataset.datasets=')
    # åŒ¹é…ç»“æŸçš„ç´¢å¼•ï¼ˆé•¿åº¦ï¼‰
    end_index = result.find('EE.dataset.customized')
    #   å°†ç»“æœå»æ‰æ‰€æœ‰ï¼›å·ï¼Œè¿˜åŸä¸ºjsonå‹å­—ç¬¦ä¸²
    json_data = result[star_index:end_index].replace(';', '').replace('\n', '')
    print(json_data)
    #   å˜æˆjsonå¯¹è±¡
    json_obj = json.dumps(json_data)
    #   æ¥ä¸‹æ¥è§£æjsonæ‹¿åˆ°å½±åƒåˆ—è¡¨æ•°æ®


#   å‡å°‘äººå·¥å‡ºé”™ï¼Œæ¨¡ä»¿java switch caseè¯­å¥å®ç°è½¬æ¢
def cloud_amount(request_var: str) -> str:
    assert isinstance(request_var, str)
    return {
        'all': '',
        '10%': '0',
        '20%': '1',
        '30%': '2',
        '40%': '3',
        '50%': '4',
        '60%': '5',
        '70%': '6',
        '80%': '7',
        '90%': '8',
        '100%': '9',
    }.get(request_var, 'error')


#   ç”±äºä¸‹è½½æ—¶é—´è¾ƒé•¿ï¼Œæ‰€ä»¥é€‰æ‹©åˆ†æ®µä¸‹è½½å¹¶æä¾›æç¤º
def download_redirect(file_name: str, request_url: str, req_number: int) -> None:
    assert isinstance(file_name, str) and isinstance(request_url, str)
    assert isinstance(req_number, int)
    with closing(conn.get(request_url, stream=True)) as response:
        # å•æ¬¡è¯·æ±‚æœ€å¤§å€¼
        chunk_size = 1024
        # å†…å®¹ä½“æ€»å¤§å°
        content_size = int(response.headers['content-length'])
        data_count = 0
        with open(file_name, "wb") as code:
            for data in response.iter_content(chunk_size=chunk_size):
                code.write(data)
                data_count = data_count + len(data)
                now_jd = (data_count / content_size) * 100
                print("\ræ­£åœ¨ä¸‹è½½ç¬¬[%d]ä¸ªæ–‡ä»¶ï¼Œæ–‡ä»¶ä¸‹è½½è¿›åº¦ï¼š%d%%(%d/%d) - %s" % (
                    req_number, now_jd, data_count, content_size, request_url), end=" ")


def img_main():
    # #   ç™»é™†çš„è´¦å·
    # username = sys.argv[1]
    # #   ç™»é™†çš„å¯†ç 
    # password = sys.argv[2]
    # #   shpæ–‡ä»¶è·¯å¾„
    # file_path = sys.argv[3]
    # #   å½±åƒå¼€å§‹æ—¶é—´
    # img_data_start = sys.argv[4]
    # #   å½±åƒç»“æŸæ—¶é—´
    # img_data_end = sys.argv[5]
    # #   é€‰æ‹©äº‘é‡
    # img_cloud_amount = sys.argv[6]

    # ç™»é™†çš„è´¦å·
    username = 'xinghengkeji'
    # ç™»é™†çš„å¯†ç 
    password = 'qwe123456'
    # éœ€è¦ä¸‹è½½çš„å½±åƒçš„shpæ–‡ä»¶
    file_path = './0404_02.zip'
    #   å½±åƒå¼€å§‹æ—¶é—´
    img_data_start = '11/15/2017'
    #   å½±åƒç»“æŸæ—¶é—´
    img_data_end = '01/10/2018'
    #   äº‘é‡é€‰æ‹©
    img_cloud_amount = '30%'
    # ç™»é™†æ¨¡å—
    result_str = login(username, password)
    print(f'\ræ¬¢è¿{result_str}ç™»å½•')
    # æ¡†é€‰åœ°å—å’Œé€‰æ‹©æ—¥æœŸ
    stage_1(file_path, img_data_start, img_data_end)
    # # é€‰æ‹©æ•°æ®ç±»å‹
    stage_2()
    # # é€‰æ‹©äº‘é‡
    stage_3(cloud_amount(img_cloud_amount))
    # é€‰æ‹©è¦ä¸‹è½½çš„å½±åƒ
    img_id_list = stage_4_img_list()
    print(f'éœ€è¦ä¸‹è½½çš„å½±åƒ å…±è®¡[{len(img_id_list)}] - {img_id_list}')
    # ä¸‹è½½å½±åƒ
    download_img(img_id_list)


# å¯åŠ¨æ–¹æ³•
if __name__ == '__main__':
    img_main()
